# Лабораторная работа 2. Линейные модели
Условие задач в IPYNB-файле. Данные для лабораторной в CSV. файле.

Сдавать задачи нужно в [Контест](https://contest.yandex.ru/contest/52933/). Что именно сдавать написано в соответствующих местах в ноутбуке.

# Task A. Простая предобработка (1 балл)
<table>
    <tbody>
        <tr>
            <td>Ограничение времени<br>
                Ограничение памяти<br>
                Ввод<br>
                Вывод<br>
            </td>
            <td valign="top">20 секунд<br>
                             256Mb<br>
                             no input<br>
                             tests.log<br>
            </td>
        </tr>
    </tbody>
</table>

В этом пункте нужно сделать базовую предобработку данных, которая состоит в следующем:
 * сохранение лишь заданных при инициализации фичей;
 * нормализация этих фичей (с помощью sklearn.preprocessing.StandardScaler)

Несколько важных замечаний:
 1. Ваш класс должен наследоваться от TransformerMixin;
 2. Метод ﬁt должен возвращать сам объект класса;
 3. Во время инициализации в качестве аргумента ваш класс должен получать список с именами колонок, которые нужно взять из общего датафрейма. Если этот аргумент не передан, то берутся все колонки.
 4. Метод transform должен получать на вход pd.Dataframe, а возвращать np.ndarray.
 
Заготовка класса в ноутбуке; пожалуйста, воспользуйтесь ей.

# Task B. Логарифмическая метрика (1 балл)
<table>
    <tbody>
        <tr>
            <td>Ограничение времени<br>
                Ограничение памяти<br>
                Ввод<br>
                Вывод<br>
            </td>
            <td valign="top">20 секунд<br>
                             256Mb<br>
                             no input<br>
                             tests.log<br>
            </td>
        </tr>
    </tbody>
</table>

В этой задаче вам нужно реализовать метрику  
```python
def root_mean_squared_logarithmic_error(y_true, y_pred):  
    # Your code here  
    pass  
```
которая вычисляется по следующей формуле:  
$$Metric = \sqrt{\frac{1}{N} \times \sum_{i=1} \left[\log y_i - \log \widehat{y_i}\right]^2}$$  
Чтобы не было проблем с отрицательными предсказаниями, все предсказания, которые меньше некоторого порога $a_{min}$  указываемого при инициализации (по умолчанию $a_{min} = 1$) нужно заменять этим порогом $ \widehat{y_i} \leftarrow \max(\widehat{y_i}, a_{min}) $, после чего уже подавать в формулу с логарифмами.  
Ваша функция должна выдавать ошибку, если среди истинных таргетов $y_i$ есть отрицательные числа.

# Task C. Экспоненциальная регрессия (1 балл)
<table>
    <tbody>
        <tr>
            <td>Ограничение времени<br>
                Ограничение памяти<br>
                Ввод<br>
                Вывод<br>
            </td>
            <td valign="top">20 секунд<br>
                             256Mb<br>
                             no input<br>
                             tests.log<br>
            </td>
        </tr>
    </tbody>
</table>

В этом задании нужно создать класс с данными свойствами:  
 1. Класс должен называться ExponentialLinearRegression
 2. Класс должен иметь такой же fit-predict интерфейс, как и было до этого. На вход он получает оригинальные X и Y, а уже внутри происходит логарифмирование или экспоненциирование.
 3. Внутри этой модели будет работать [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html). Хочется, чтобы этому классу можно было передавать аргументы инициализации с помощью *args и **kwargs
 4. Чтобы потом этот класс можно было использовать в [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) в следующих пунктах у него должны быть реализованы 5 методов  
    * __init__(self, *args, **kwargs) все полученные аргументы передаются дальше в Ridge.
    * fit(self, X, Y) обучает класс, возвращает self.
    * predict(self, X) делает предсказание.
    * get_params(deep=True) возвращает dict с параметрами модели. Больше подробностей [здесь](https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html)
    * set_params(**params) передает нужные параметры в модель. Больше подробностей [здесь](https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html)
Есть два подхода к тому как сделать все нужные методы:
 * Отнаследоваться от класса Ridge и переопределить методы fit и predict, внутри вызывая super() от отцовского класса.
 * Отнаследоваться от класса RegressorMixin и внутренним атрибутом класса сделать Ridge. Тогда все методы нужно будет писать руками.

# Task D. Оценка по кросс-валидации
Введите число, округлённое до целого:  
 * MAE обычной линейной регрессии (без регуляризации и с дефолтными параметрами) на объединении обучающей и тестовой выборок, посчитанную по кросс-валидации.

В качестве параметра cv в кросс-валидации вам нужно указать KFold(n_splits=5, shuﬄe=True, random_state=42)

## Формат вывода
Округлите ответ до целого

## Примечания
Не пишите в ответе никаких лишних символов, это может привести к тому, что система посчитает ответ неверным. Например, если 42 — правильный ответ на задачу, то не нужно писать так:  
_Ответ: 42_  
Нужно так:  
42  

__Внимание! Если вы отправите ответ несколько раз, проверена будет только последняя посылка.__

# Task E. Самописная линейная регрессия
<table>
    <tbody>
        <tr>
            <td>Ограничение времени<br>
                Ограничение памяти<br>
                Ввод<br>
                Вывод<br>
            </td>
            <td valign="top">60 секунд<br>
                             256Mb<br>
                             no input<br>
                             tests.log<br>
            </td>
        </tr>
    </tbody>
</table>

В этом задании вам нужно загрузить класс $L2$-регуляризованной линейной регрессии, обучающийся с помощью стохастического градиентного спуска. Важные требования:  
 1. Класс должен называться SGDLinearRegressor
 2. Класс должен быть отнаследован от sklearn-овского класса RegressorMixin
 3. Класс должен инициализироваться со следующими гиперпараметрами: 
    * lr-learning rate. Длина шага градиентного спуска 
    * regularization - коэффициент $\lambda$ регуляризации
    * delta_converged - устанавливает условие окончание обучение. В тот момент, когда норма разности весов на соседних шагах градиентного спуска меньше, чем delta_converged, алгоритм прекращает обновлять веса
    * max_steps - максимальное число шагов градиентного спуска 
    * batch_size - размер батча
 4. Реализуйте __стохастический__ градиентный спуск, в котором батчи размера batch_size генерируются не детерминированно, а с той или иной степенью случайности. Вы можете либо на каждом шаге случайно генерировать батч, либо каждую эпоху перемешивать обучающую выборку и итерироваться последовательными батчами по перемешанной выборке.

Вот заготовка класса:  
```python
class SGDLinearRegressor(RegressorMixin):
    def __init__(self, lr=0.01, regularization=1., delta_converged=1e-2, max_steps=1000, batch_size=64):
        self.lr = lr
        self.regularization = regularization
        self.max_steps = max_steps
        self.delta_converged = delta_converged
        self.batch_size = batch_size

        self.W = None
        self.b = None

    def fit(self, X, Y):
        # <Your code here>
        pass

    def predict(self, X):
        # <Your code here>
        pass
```

# Task F. Пайплайн
<table>
    <tbody>
        <tr>
            <td>Ограничение времени<br>
                Ограничение памяти<br>
                Ввод<br>
                Вывод<br>
            </td>
            <td valign="top">60 секунд<br>
                             256Mb<br>
                             input.data<br>
                             tests.log<br>
            </td>
        </tr>
    </tbody>
</table>

В этом задании вам нужно загрузить функцию:    
```python
def make_ultimate_pipeline():
    # <YOUR CODE HERE>
```
которая возвращает модель-пайплайн (в контесте будет проверено, что это действительно пайплайн). Пожалуйста, не забывайте, что все классы и переменные, которые вам нужны, должны быть определены и заимпортированы в вашей посылке.  
Ваша оценка будет равна 0 (и вы получите WA), если ваш скор больше 0.19, и 
```python
np.round(1 + min(1, (0.19 - metric_value)/(0.19 - 0.15)), 2)
```
если ваш скор меньше 0.19.